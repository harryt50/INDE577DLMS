{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.2 Programming a Deep Neural Network from Scratch with Python\n",
    "\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_6/Lecture_6_2.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## MNIST Data Set\n",
    "The [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database) consists of 70,000 images of hand written digits, 60,000 of which are typically used as labeled training examples and the other 10,000 used for testing your learning model on. The following picture represent a sample of some of the images.\n",
    "\n",
    "<img src=\"MnistExamples.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can load this dataset with the ```tensorflow.keras``` package. Load this data by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/Users/rdavila/tensorflow-test/env/bin/python\"\n  * The NumPy version is: \"1.22.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib\n  Referenced from: /Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so\n  Reason: tried: '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multiarray\n\u001b[1;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/multiarray.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/overrides.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_multiarray_umath\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m getargspec\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib\n  Referenced from: /Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so\n  Reason: tried: '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/rdavila/Documents/Teaching_ML/Data_Science_and_Machine_Learning_Spring_2022/Lecture_6/Lecture_6_2.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rdavila/Documents/Teaching_ML/Data_Science_and_Machine_Learning_Spring_2022/Lecture_6/Lecture_6_2.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#!pip install tensorflow\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rdavila/Documents/Teaching_ML/Data_Science_and_Machine_Learning_Spring_2022/Lecture_6/Lecture_6_2.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rdavila/Documents/Teaching_ML/Data_Science_and_Machine_Learning_Spring_2022/Lecture_6/Lecture_6_2.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rdavila/Documents/Teaching_ML/Data_Science_and_Machine_Learning_Spring_2022/Lecture_6/Lecture_6_2.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/tensorflow/python/eager/context.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m---> 26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/numpy/__init__.py:144\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init\n\u001b[0;32m--> 144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m    145\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/__init__.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m], sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m], sys\u001b[39m.\u001b[39mexecutable,\n\u001b[1;32m     48\u001b[0m         __version__, exc)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m     50\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m envkey \u001b[39min\u001b[39;00m env_added:\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/Users/rdavila/tensorflow-test/env/bin/python\"\n  * The NumPy version is: \"1.22.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib\n  Referenced from: /Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so\n  Reason: tried: '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/Users/rdavila/tensorflow-test/env/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the keras module to import the necessary data \n",
    "(train_X, train_y), (test_X, test_y) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The data structures ```train_x``` and ```test_x``` are stored as 3 dimensional tensors. \n",
    " \n",
    "<img src=\"order-3-tensor.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "This can be varified by finding the shape of these variables. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run inplace of the cell directly above\n",
    "print(f\"np.shape(train_X) ={np.shape(train_X)}\")\n",
    "print(f\"np.shape(test_X) = {np.shape(test_X)} \\n\")\n",
    "\n",
    "print(f\"np.shape(train_X[0]) = {np.shape(train_X[0])}\")\n",
    "print(f\"np.shape(test_X[0]) = {np.shape(test_X[0])} \\n\")\n",
    "\n",
    "print(f\"train_X[0] = {train_X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Each image is comprised of a $28\\times 28$ grey scaled grid of pixel values. These values are floating point numbers in the interval $(0,1)$, where darker pixels will have values closer to $1$ and lighter pixels will have values closer to $0$. The following image represents one such example. \n",
    "\n",
    "<img src=\"MNIST-Matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "We can view the image of one of these matrices by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_y[0] = } \\n\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image Flattening\n",
    "\n",
    "Simple **dense neural networks** pass feature vectors into the 0-th (the first) layer of the computational graph represented by the neural network structure as column vectors. This 0-th layer is essentially the same as with single neuron models. In order to feed our images into such a network we must **flatten** the matrix into a column vector.\n",
    "\n",
    "<img src=\"flatten.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can do this for each image matrix that we are considering by calling the ```flatten()``` method together with the ```reshape(784, 1)``` method to insure it is a column vector. Note, that each image matrix is 28 by 28, and so, $784 = 28 \\times 28$ is the number of rows in the flattened matrix column vector. The numerical values in the flattened training and testing data matrices vary between 0 and 255. These large differences in possible values can lead to problems when training the weights and the biases of the neural network. A quick and dirty fix will be to scale all data to belong in the interval $(0, 1)$, i.e., divide the entries by the largest possible value; in this case 255. \n",
    "\n",
    "## One-Hot Encoding \n",
    "\n",
    "<img src=\"onehot.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following code scales our training and testing data, reshapes our images and stores them in new variable names, and one-hot encodes the labels. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data for training. \n",
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "\n",
    "# Flatten the training images into coloumn vectors. \n",
    "flat_train_X = []\n",
    "# One hot encode the training labels\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(train_X, train_y):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "   \n",
    "\n",
    "# Do the same for the testing data \n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(test_X, test_y):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the Network Architecture \n",
    "For our purposes, we will build a multilayered **fully connected**, or **dense**, neural network with $L$ layers, $784$ input notes, $L-2$ hidden layers of arbitrary size, and $10$ output nodes. \n",
    "\n",
    "<img src=\"multilayerPerceptron.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "For our activation function, we will use the sigmoid function:\n",
    "\n",
    "* Sigmoid Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "For our cost function, we will use the Mean Sqaure Error cost:\n",
    "$$\n",
    "C(W, b) = \\frac{1}{2}\\sum_{k=1}^{10}(\\hat{y}^{(i)}_k - y^{(i)}_k)^2.\n",
    "$$\n",
    "\n",
    "Our goal will be to write a custom Python class implementing our desired structure. However, before doing so, we first sequentually write functions to better understand the process of programming the following:\n",
    "\n",
    "* Initializing the weights and biases of each layer\n",
    "* The feedforward phase\n",
    "* Calculation of the cost function\n",
    "* Calculation of the gradient\n",
    "* Iterating stochastic gradient descent\n",
    "\n",
    "First we will define our sigmoid activation function, its derivative, and the mean squared error for a single instance of training data. Do this by running the following code. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return .5*sum((a[i] - y[i])**2 for i in range(10))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we will write a custom function to initialize the weight matrices and bias column vectors for a dense neural network. Do this by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "    # The following Python lists will contain numpy matrices\n",
    "    # connected the layers in the neural network \n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        # The scalling factor is something I found in a research paper :)\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "    \n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedforward Phase\n",
    "\n",
    "For $\\ell = 1, \\dots, L$, each layer $\\ell$ in our network will have two phases, the preactivation phase $$\\mathbf{z}^{\\ell} = W^{\\ell}\\mathbf{a}^{\\ell-1} + \\mathbf{b}^{\\ell},$$ and postactivation phase $$\\mathbf{a}^{\\ell} = \\sigma(\\mathbf{z}^{\\ell}).$$ The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through an activation function elementwise. Note $\\mathbf{a}^0 = \\mathbf{x}^{(i)}$, where $\\mathbf{x}^{(i)}$ is the current input data into our network. \n",
    "\n",
    "We can test our activation functions and matrix dimensions by running the following code which manually implements the feedforward process on a neural network with the given dimensions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()\n",
    "\n",
    "xi = flat_train_X[0]\n",
    "yi = onehot_train_y[0]\n",
    "a0 = xi\n",
    "\n",
    "print(f\"np.shape(a0) = {np.shape(a0)} \\n\")\n",
    "\n",
    "z1 = W[1] @ a0 + B[1]\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "print(f\"np.shape(W[1]) = {np.shape(W[1])}\")\n",
    "print(f\"np.shape(z1) = {np.shape(z1)}\")\n",
    "print(f\"np.shape(a1) = {np.shape(a1)} \\n\")\n",
    "\n",
    "z2 = W[2] @ a1 + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(f\"np.shape(W[2]) = {np.shape(W[2])}\")\n",
    "print(f\"np.shape(z2) = {np.shape(z2)}\")\n",
    "print(f\"np.shape(a2) = {np.shape(a2)} \\n\")\n",
    "\n",
    "z3 = W[3] @ a2 + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "y_hat = a3\n",
    "print(f\"np.shape(W[3]) = {np.shape(W[3])}\")\n",
    "print(f\"np.shape(z3) = {np.shape(z3)}\")\n",
    "print(f\"np.shape(a3) = {np.shape(a3)} \\n\")\n",
    "\n",
    "\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Obviously we could wrap all these lines of code into a ```for```-loop and place it in a callable function. The following code does precisely this. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, B, xi, predict_vector = False):\n",
    "    Z = [[0.0]]\n",
    "    A = [xi]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "        \n",
    "    if predict_vector == False:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1]\n",
    "\n",
    "def predict(W, B, xi):\n",
    "    _, A = forward_pass(W, B, xi)\n",
    "    return np.argmax(A[-1])\n",
    "\n",
    "y_hat = forward_pass(W, B, flat_train_X[0], predict_vector=True)\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let us next write a custom function for making a prediction on a random data point, as well as write a mean squared error function that computes the error over an entire set of features and labels.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_experiment(W, B, data_features, data_labels):\n",
    "    i = np.random.randint(len(data_features))\n",
    "    print(f\"Actual label: {np.argmax(data_labels[i])}\")\n",
    "    print(f\"Predicted label: {predict(W, B, data_features[i])}\")\n",
    "    \n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    m = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        m+=1\n",
    "    return cost/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE(W, B, flat_train_X, onehot_train_y) = {MSE(W, B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "random_experiment(W, B, flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backpropogation Phase with Stochastic Gradient Descent \n",
    "We are now ready to define a custom Python ```DenseNetwork``` class which initializes the weights and bias for the network, and implements stochastic gradient descent shown below:\n",
    "\n",
    "1. For each $i = 1, \\dots, N$.\n",
    "2. Feedforward $\\mathbf{x}^{(i)}$ into the network. \n",
    "3. Compute $\\delta^{L} = \\nabla_aC\\otimes \\sigma'(\\mathbf{z}^{L})$.\n",
    "4. For $\\ell = L-1, \\dots, 1$, compute $\\delta^{\\ell} = \\big ( (\\mathbf{w}^{\\ell + 1})^{T} \\delta^{\\ell + 1} \\Big )\\otimes \\sigma'(\\mathbf{z}^{\\ell})$.\n",
    "5. For $\\ell = L, L-1, \\dots, 1$, \n",
    "\n",
    "$$\n",
    "w^{\\ell} \\leftarrow w^{\\ell} - \\alpha \\delta^{\\ell}(\\mathbf{a}^{\\ell-1})^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{\\ell} \\leftarrow b^{\\ell} - \\alpha \\delta^{\\ell}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers = self.layers)\n",
    "\n",
    "    def train(self, X_train, y_train, alpha = 0.046, epochs = 4):\n",
    "        # Print the initial mean squared error\n",
    "        self.errors_ = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors_[0]}\")\n",
    "\n",
    "        # Find your sample size\n",
    "        sample_size = len(X_train)\n",
    "\n",
    "        # Find the number of non-input layers.\n",
    "        L = len(self.layers) - 1\n",
    "\n",
    "        # For each epoch perform stochastic gradient descent. \n",
    "        for k in range(epochs):\n",
    "            # Loop over each (xi, yi) training pair of data.\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                # Compute the output error \n",
    "                output_error = (A[L] - yi)*d_sigmoid(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function \n",
    "                # is non-inclusive. \n",
    "                for i in range(L-1, 0, -1):\n",
    "                    # Compute the node errors at each hidden layer\n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*d_sigmoid(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient \n",
    "                # descent. \n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the user the cost over all training examples\n",
    "            self.errors_.append(MSE(self.W, self.B, X_train, y_train))   \n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors_[-1]}\")\n",
    "    \n",
    "\n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network with 784 input nodes, two hidden layers with 60 nodes each \n",
    "# and a output layer with 10 nodes. \n",
    "net = DenseNetwork(layers = [784, 120, 145, 120, 10])\n",
    "\n",
    "# Check the mean squared error before training \n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "# Make a random prediction before training\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your network with stochastic gradient descent!\n",
    "net.train(flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean squared error over the training process \n",
    "plt.figure(figsize = (10, 8))\n",
    "epochs = range(len(net.errors_))\n",
    "plt.plot(epochs, net.errors_, marker = \"o\")\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Network MSE During Training\", fontsize = 16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification Error\n",
    "Let us now calculate the classification percentage on the testing data for our trained dense neural network. Recall that this is simply the number of correct labels divided by the total number of data points:\n",
    "\n",
    "This can be done by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the classification accuracy of our trained model on the test data (I bet we did well!)\n",
    "sum([int(net.predict(x) == y) for x, y in zip(flat_test_X, test_y)])/len(onehot_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "60d347b1b00434e3109b3f59c5a731168507f4d33f1c02fc1f75d9db6931c10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
